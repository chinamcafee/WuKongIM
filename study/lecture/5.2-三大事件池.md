# 5.2 三大事件池

> **本节目标**：深入理解WuKongIM三大事件池的设计与实现，掌握事件池的工作机制

---

## 📋 目录
1. [事件池架构概览](#事件池架构概览)
2. [UserEventPool用户事件池](#usereventpool用户事件池)
3. [ChannelEventPool频道事件池](#channeleventpool频道事件池)
4. [PusherEventPool推送事件池](#pushereventpool推送事件池)
5. [事件池工作机制](#事件池工作机制)
6. [性能优化与监控](#性能优化与监控)

---

## 1️⃣ 事件池架构概览

### **A. 为什么需要三个事件池？**

**分层设计理念**：
```
单一事件池问题：
├─ 所有事件混在一起
├─ 轻重不分（连接事件 vs 消息存储）
├─ 故障影响大（一个Handler卡住，全部阻塞）
└─ 资源竞争（无法独立调优）

三个事件池优势：
✅ 职责分离（连接、消息、推送）
✅ 故障隔离（互不影响）
✅ 资源隔离（独立的工作协程池）
✅ 性能调优（针对性优化）
```

---

### **B. 三大事件池职责**

```
┌─────────────────────────────────────────────────────┐
│  UserEventPool（用户事件池）                         │
│  职责：处理用户连接相关事件                           │
│  ├─ EventConnect：连接建立                           │
│  ├─ EventOnSend：消息发送                            │
│  ├─ EventRecvAck：接收确认                           │
│  └─ EventConnClose：连接关闭                         │
│  特点：轻量、高频、快速响应                           │
└─────────────────────────────────────────────────────┘
                       ↓ 产生频道事件
┌─────────────────────────────────────────────────────┐
│  ChannelEventPool（频道事件池）                       │
│  职责：处理频道消息相关事件                           │
│  ├─ EventChannelDistribute：消息分发                 │
│  ├─ EventChannelStore：消息存储                      │
│  └─ EventChannelSync：消息同步                       │
│  特点：涉及存储、共识，相对较重                       │
└─────────────────────────────────────────────────────┘
                       ↓ 产生推送事件
┌─────────────────────────────────────────────────────┐
│  PusherEventPool（推送事件池）                        │
│  职责：处理消息推送相关事件                           │
│  ├─ EventPushOnline：在线推送                        │
│  ├─ EventPushOffline：离线推送                       │
│  └─ EventPushBatch：批量推送                         │
│  特点：网络I/O密集，订阅者可能很多                    │
└─────────────────────────────────────────────────────┘
```

---

### **C. 事件流转链路**

```
完整消息流转：

1. 客户端发送SEND包
    ↓
2. wknet.onData() 接收数据
    ↓
3. 生成 UserContext{EventOnSend}
    ↓
4. UserEventPool.AddEvent() ← 进入用户事件池
    ↓
5. OnSendHandler处理
    ├─ 权限校验
    ├─ Webhook回调
    └─ 生成 ChannelContext{EventChannelDistribute}
        ↓
6. ChannelEventPool.AddEvent() ← 进入频道事件池
    ↓
7. DistributeHandler处理
    ├─ 消息存储
    ├─ Raft共识
    └─ 生成 PushContext{EventPushOnline}
        ↓
8. PusherEventPool.AddEvent() ← 进入推送事件池
    ↓
9. PushOnlineHandler处理
    ├─ 查找订阅者
    └─ 推送消息
        ↓
10. 订阅者收到RECV包
```

---

## 2️⃣ UserEventPool 用户事件池

### **A. 设计目标**

```
用户事件池特点：
├─ 高频率：每秒百万级事件
├─ 轻量级：大部分Handler耗时<1ms
├─ 用户隔离：每个用户独立队列
└─ 快速响应：立即处理

性能目标：
├─ 延迟：P99 < 5ms
├─ 吞吐：100万事件/秒
└─ 内存：控制在合理范围
```

---

### **B. 核心数据结构**

**代码位置**：`internal/user/reactor.go`

```go
type UserReactor struct {
    // 用户事件队列（按UID分片）
    userQueues sync.Map  // map[string]*UserQueue

    // 工作协程池
    workers []*Worker

    // 配置
    workerCount  int           // 工作协程数量
    queueSize    int           // 队列大小
    stopped      atomic.Bool   // 停止标志
}

// 用户队列
type UserQueue struct {
    uid      string            // 用户ID
    queue    chan *Event       // 事件队列
    mu       sync.Mutex        // 锁
    processing atomic.Bool     // 是否正在处理
}
```

---

### **C. 事件添加流程**

**代码位置**：`internal/user/reactor.go:AddEvent()`

```go
func (r *UserReactor) AddEvent(uid string, event *Event) {
    // 1. 获取或创建用户队列
    queue := r.getOrCreateQueue(uid)

    // 2. 尝试写入队列
    select {
    case queue.queue <- event:
        // 成功写入
    default:
        // 队列满，记录日志
        log.Warn("user queue full", zap.String("uid", uid))
        return
    }

    // 3. 尝试触发处理
    r.tryProcess(queue)
}

func (r *UserReactor) getOrCreateQueue(uid string) *UserQueue {
    // 从缓存获取
    if q, ok := r.userQueues.Load(uid); ok {
        return q.(*UserQueue)
    }

    // 创建新队列
    queue := &UserQueue{
        uid:   uid,
        queue: make(chan *Event, r.queueSize),  // 默认1000
    }

    // 存储到缓存
    actual, loaded := r.userQueues.LoadOrStore(uid, queue)
    if loaded {
        // 其他协程已创建，使用已有的
        return actual.(*UserQueue)
    }

    return queue
}
```

---

**按用户分片的优势**：
```
为什么按UID分片？

1. 顺序保证：
   ├─ 同一用户的事件按FIFO顺序处理
   ├─ EventOnSend → EventRecvAck 顺序不乱
   └─ 避免并发冲突

2. 负载均衡：
   ├─ 不同用户的事件可以并行处理
   ├─ 充分利用多核CPU
   └─ 100万用户 = 100万个独立队列

3. 故障隔离：
   ├─ 某个用户的队列满了，不影响其他用户
   ├─ 某个用户的Handler慢，不影响其他用户
   └─ 单点故障影响最小化

示例：
用户A发送1000条消息（同一队列，顺序处理）
用户B发送1000条消息（独立队列，并行处理）
```

---

### **D. 事件处理流程**

```go
func (r *UserReactor) tryProcess(queue *UserQueue) {
    // 1. 使用CAS检查是否已在处理
    if !queue.processing.CompareAndSwap(false, true) {
        return  // 已有协程在处理，直接返回
    }

    // 2. 提交到工作协程池
    r.submitToWorker(queue)
}

func (r *UserReactor) submitToWorker(queue *UserQueue) {
    // 选择一个Worker（负载均衡）
    worker := r.selectWorker(queue.uid)

    // 提交任务
    worker.Submit(func() {
        r.processQueue(queue)
    })
}

func (r *UserReactor) processQueue(queue *UserQueue) {
    defer queue.processing.Store(false)

    for {
        // 1. 尝试获取事件（非阻塞）
        select {
        case event := <-queue.queue:
            // 2. 执行事件Handler
            ExecuteUserEvent(event)

        default:
            // 队列为空，退出
            return
        }

        // 3. 批量处理优化：连续处理多个事件
        // （避免频繁的协程调度）
    }
}
```

---

### **E. 支持的事件类型**

**代码位置**：`internal/eventbus/event.go`

```go
const (
    // 连接相关
    EventConnect       EventType = 1  // 连接建立
    EventOnConnect     EventType = 2  // 认证完成
    EventConnWriteFrame EventType = 3  // 写入Frame
    EventConnClose     EventType = 4  // 连接关闭

    // 消息相关
    EventOnSend        EventType = 10 // 消息发送
    EventRecvAck       EventType = 11 // 接收确认
    EventSendAck       EventType = 12 // 发送确认

    // 订阅相关
    EventSub           EventType = 20 // 订阅频道
    EventUnsub         EventType = 21 // 取消订阅
)
```

---

### **F. 事件Handler注册**

**代码位置**：`internal/server/server.go:initUserHandlers()`

```go
func (s *Server) initUserHandlers() {
    // 注册连接建立Handler
    RegisterUserHandlers(EventConnect,
        s.connectHandler.Handle,
    )

    // 注册消息发送Handler
    RegisterUserHandlers(EventOnSend,
        s.permissionHandler.Check,   // 权限校验
        s.webhookHandler.OnSend,     // Webhook回调
        s.onSendHandler.Handle,      // 核心处理
    )

    // 注册接收确认Handler
    RegisterUserHandlers(EventRecvAck,
        s.recvAckHandler.Handle,
    )

    // 注册连接关闭Handler
    RegisterUserHandlers(EventConnClose,
        s.closeHandler.Handle,
    )
}
```

**责任链模式**：
```
事件处理流程（责任链）：

EventOnSend
    ↓
Handler 1: permissionHandler.Check()
    ├─ 成功 → 继续下一个Handler
    └─ 失败 → 返回错误，终止链

Handler 2: webhookHandler.OnSend()
    ├─ 成功 → 继续
    └─ 失败 → 返回错误

Handler 3: onSendHandler.Handle()
    └─ 核心业务逻辑

特点：
✅ Handler按顺序执行
✅ 某个Handler失败，终止后续
✅ 易于扩展（插入新Handler）
```

---

## 3️⃣ ChannelEventPool 频道事件池

### **A. 设计目标**

```
频道事件池特点：
├─ 中等频率：每秒10万级事件
├─ 中等重量：涉及存储、Raft共识
├─ 频道隔离：每个频道独立队列
└─ 可靠性优先：保证消息不丢失

性能目标：
├─ 延迟：P99 < 50ms
├─ 吞吐：10万事件/秒
└─ 可靠性：99.99%
```

---

### **B. 核心数据结构**

**代码位置**：`internal/channel/reactor.go`

```go
type ChannelReactor struct {
    // 频道事件队列（按ChannelID分片）
    channelQueues sync.Map  // map[string]*ChannelQueue

    // 工作协程池
    workers []*Worker

    // 配置
    workerCount int
    queueSize   int
    stopped     atomic.Bool
}

type ChannelQueue struct {
    channelID   string
    channelType uint8
    queue       chan *Event
    mu          sync.Mutex
    processing  atomic.Bool
}
```

---

### **C. 事件添加流程**

```go
func (r *ChannelReactor) AddEvent(channelID string, channelType uint8, event *Event) {
    // 1. 构造频道Key
    key := channelKey(channelID, channelType)

    // 2. 获取或创建频道队列
    queue := r.getOrCreateQueue(key, channelID, channelType)

    // 3. 写入队列
    select {
    case queue.queue <- event:
        // 成功
    default:
        log.Warn("channel queue full",
            zap.String("channelID", channelID),
            zap.Uint8("channelType", channelType))
        return
    }

    // 4. 触发处理
    r.tryProcess(queue)
}

func channelKey(channelID string, channelType uint8) string {
    return fmt.Sprintf("%s-%d", channelID, channelType)
}
```

---

### **D. 支持的事件类型**

```go
const (
    // 频道消息事件
    EventChannelDistribute EventType = 100  // 消息分发
    EventChannelStore      EventType = 101  // 消息存储
    EventChannelSync       EventType = 102  // 消息同步

    // 频道管理事件
    EventChannelCreate     EventType = 110  // 频道创建
    EventChannelUpdate     EventType = 111  // 频道更新
    EventChannelDelete     EventType = 112  // 频道删除

    // 订阅者管理
    EventSubscriberAdd     EventType = 120  // 添加订阅者
    EventSubscriberRemove  EventType = 121  // 移除订阅者
)
```

---

### **E. 事件Handler注册**

**代码位置**：`internal/server/server.go:initChannelHandlers()`

```go
func (s *Server) initChannelHandlers() {
    // 注册消息分发Handler
    RegisterChannelHandlers(EventChannelDistribute,
        s.distributeHandler.PreCheck,    // 预检查
        s.distributeHandler.Store,       // 存储消息
        s.distributeHandler.Replicate,   // Raft复制
        s.distributeHandler.PostProcess, // 后处理
    )

    // 注册消息存储Handler
    RegisterChannelHandlers(EventChannelStore,
        s.storeHandler.Handle,
    )
}
```

---

### **F. 频道事件处理特点**

**1. Raft共识**
```go
func (h *DistributeHandler) Replicate(ctx *ChannelContext) error {
    // 1. 提议到Raft
    proposal := &Proposal{
        Type: ProposalTypeMessage,
        Data: ctx.Message,
    }

    // 2. 等待Raft提交（阻塞）
    err := h.raftServer.Propose(ctx.ChannelID, proposal)
    if err != nil {
        return err
    }

    // 3. 等待应用到状态机
    <-ctx.ApplyChan

    return nil
}

特点：
├─ 涉及网络通信（Leader → Follower）
├─ 需要等待多数派确认
├─ 延迟较高（10-50ms）
└─ 保证强一致性
```

---

**2. 消息存储**
```go
func (h *StoreHandler) Handle(ctx *ChannelContext) error {
    // 1. 批量写入（性能优化）
    batch := h.db.NewBatch()

    for _, msg := range ctx.Messages {
        // 2. 构造存储Key
        key := messageKey(msg.ChannelID, msg.MessageSeq)

        // 3. 序列化消息
        data, _ := msg.Encode()

        // 4. 写入Batch
        batch.Put(key, data)
    }

    // 5. 提交Batch
    err := batch.Commit()
    return err
}

特点：
├─ 批量写入（减少I/O）
├─ LSM-Tree存储（PebbleDB）
├─ 异步刷盘（性能优化）
└─ 持久化保证
```

---

## 4️⃣ PusherEventPool 推送事件池

### **A. 设计目标**

```
推送事件池特点：
├─ 高频率：每秒百万级事件
├─ I/O密集：涉及大量网络写入
├─ 订阅者分散：可能分布在多个节点
└─ 尽力而为：离线不影响整体

性能目标：
├─ 延迟：P99 < 20ms
├─ 吞吐：100万推送/秒
└─ 成功率：>99%（在线推送）
```

---

### **B. 核心数据结构**

**代码位置**：`internal/pusher/reactor.go`

```go
type PusherReactor struct {
    // 推送事件队列（全局队列）
    queue chan *Event

    // 工作协程池
    workers []*Worker

    // 配置
    workerCount int
    queueSize   int
    stopped     atomic.Bool
}
```

**为什么不按用户分片？**
```
推送事件池使用全局队列：

原因：
├─ 推送是多对多（一条消息推送给多个订阅者）
├─ 无法按单一维度分片（ChannelID？UID？）
├─ 推送之间无依赖关系（可以乱序）
└─ 全局队列+多Worker并行，吞吐更高

优化：
├─ Worker数量较多（16-32个）
├─ 每个Worker并行推送
└─ 批量推送优化
```

---

### **C. 事件添加流程**

```go
func (r *PusherReactor) AddEvent(event *Event) {
    // 直接写入全局队列
    select {
    case r.queue <- event:
        // 成功
    default:
        // 队列满，丢弃（推送是尽力而为）
        log.Warn("pusher queue full")
    }
}
```

---

### **D. 支持的事件类型**

```go
const (
    // 推送事件
    EventPushOnline  EventType = 200  // 在线推送
    EventPushOffline EventType = 201  // 离线推送
    EventPushBatch   EventType = 202  // 批量推送
    EventPushAck     EventType = 203  // 推送确认
)
```

---

### **E. 推送Handler实现**

**代码位置**：`internal/pusher/handler/push_online.go`

```go
func (h *PushOnlineHandler) Handle(ctx *PushContext) error {
    // 1. 查找频道订阅者
    subscribers := h.getSubscribers(ctx.ChannelID, ctx.ChannelType)

    // 2. 过滤在线订阅者
    onlineSubscribers := h.filterOnline(subscribers)

    // 3. 构造RECV包
    recvPacket := &wkproto.RecvPacket{
        MessageID:   ctx.Message.MessageID,
        MessageSeq:  ctx.Message.MessageSeq,
        FromUID:     ctx.Message.FromUID,
        ChannelID:   ctx.ChannelID,
        ChannelType: ctx.ChannelType,
        Payload:     ctx.Message.Payload,
        Timestamp:   ctx.Message.Timestamp,
    }

    // 4. 编码
    data, _ := Proto.EncodeFrame(recvPacket, wkproto.LatestVersion)

    // 5. 批量推送
    for _, subscriber := range onlineSubscribers {
        // 查找订阅者的连接
        conns := User.ConnsByUid(subscriber.UID)

        for _, conn := range conns {
            // 写入连接（异步）
            User.WriteLocalData(conn, data)
        }
    }

    return nil
}
```

---

### **F. 推送优化技巧**

**1. 批量推送**
```go
// ❌ 低效：逐个推送
for _, subscriber := range subscribers {
    conn := getConn(subscriber.UID)
    conn.Write(data)
}

// ✅ 高效：批量推送
type PushTask struct {
    UIDs []string
    Data []byte
}

func (h *PushHandler) BatchPush(task *PushTask) {
    // 1. 批量查询连接
    conns := h.batchGetConns(task.UIDs)

    // 2. 使用writev批量写入
    iovecs := make([][]byte, len(conns))
    for i := range iovecs {
        iovecs[i] = task.Data
    }

    // 3. 一次系统调用写入多个连接
    writev(conns, iovecs)
}
```

---

**2. 跨节点推送**
```go
func (h *PushHandler) Handle(ctx *PushContext) error {
    // 1. 按节点分组订阅者
    nodeGroups := h.groupByNode(ctx.Subscribers)

    // 2. 本地推送
    localUIDs := nodeGroups[options.G.Cluster.NodeId]
    h.pushLocal(localUIDs, ctx.Data)

    // 3. 跨节点推送
    for nodeID, uids := range nodeGroups {
        if nodeID == options.G.Cluster.NodeId {
            continue  // 跳过本地
        }

        // 发送RPC请求到其他节点
        h.pushRemote(nodeID, uids, ctx.Data)
    }

    return nil
}
```

---

## 5️⃣ 事件池工作机制

### **A. Worker池模型**

```
Worker池架构：

┌─────────────────────────────────────────┐
│  事件池                                  │
│  ├─ 事件队列（chan *Event）              │
│  └─ Worker数组                          │
└─────────────────────────────────────────┘
                ↓ 分发任务
┌─────────────────────────────────────────┐
│  Worker-0   Worker-1   Worker-2   ...   │
│     ↓          ↓          ↓             │
│  处理队列   处理队列   处理队列          │
└─────────────────────────────────────────┘

Worker实现：
type Worker struct {
    id       int
    taskChan chan func()  // 任务队列
    pool     *WorkerPool
}

func (w *Worker) Run() {
    for {
        select {
        case task := <-w.taskChan:
            // 执行任务
            task()

        case <-w.stopChan:
            return
        }
    }
}
```

---

### **B. 负载均衡策略**

**1. 轮询（Round Robin）**
```go
type WorkerPool struct {
    workers []*Worker
    next    atomic.Uint64
}

func (p *WorkerPool) Submit(task func()) {
    // 轮询选择Worker
    idx := p.next.Add(1) % uint64(len(p.workers))
    worker := p.workers[idx]

    worker.taskChan <- task
}

优点：
✅ 简单高效
✅ 分布均匀
```

---

**2. 哈希（Hash）**
```go
func (p *WorkerPool) SubmitWithKey(key string, task func()) {
    // 根据Key哈希选择Worker
    hash := fnv.New64a()
    hash.Write([]byte(key))
    idx := hash.Sum64() % uint64(len(p.workers))

    worker := p.workers[idx]
    worker.taskChan <- task
}

优点：
✅ 同Key的任务分配到同一Worker
✅ 保证顺序性（UserEventPool使用）
```

---

### **C. 事件处理完整流程**

```
事件处理全流程：

1. 事件产生
   └─ server.onData() 收到消息

2. 事件封装
   └─ 创建UserContext对象

3. 事件发布
   └─ UserEventPool.AddEvent(uid, event)

4. 写入队列
   └─ userQueue.queue <- event

5. 触发处理
   └─ tryProcess(userQueue)

6. 提交到Worker
   └─ worker.Submit(processQueue)

7. Worker执行
   └─ for event := range queue { ... }

8. 执行Handler链
   └─ ExecuteUserEvent(event)
       ├─ Handler1.Handle()
       ├─ Handler2.Handle()
       └─ Handler3.Handle()

9. 完成
   └─ 处理下一个事件
```

---

## 6️⃣ 性能优化与监控

### **A. 性能优化技巧**

**1. 批量处理**
```go
func (r *UserReactor) processQueue(queue *UserQueue) {
    batch := make([]*Event, 0, 100)

    for {
        // 尽可能多地收集事件
        for len(batch) < 100 {
            select {
            case event := <-queue.queue:
                batch = append(batch, event)
            default:
                goto PROCESS
            }
        }

    PROCESS:
        if len(batch) == 0 {
            return
        }

        // 批量执行
        for _, event := range batch {
            ExecuteUserEvent(event)
        }

        batch = batch[:0]  // 重置（复用切片）
    }
}

优点：
✅ 减少协程调度次数
✅ 提高CPU缓存命中率
✅ 吞吐量提升30-50%
```

---

**2. 对象池复用**
```go
var eventPool = sync.Pool{
    New: func() any {
        return &Event{}
    },
}

func NewEvent(eventType EventType) *Event {
    event := eventPool.Get().(*Event)
    event.Type = eventType
    return event
}

func RecycleEvent(event *Event) {
    // 重置事件
    event.Type = 0
    event.Conn = nil
    event.Frame = nil

    eventPool.Put(event)
}

优点：
✅ 减少GC压力
✅ 内存分配减少90%+
```

---

**3. 异步写入**
```go
// ❌ 同步写入（阻塞）
func (h *Handler) Handle(ctx *UserContext) error {
    data := encodeFrame(ctx.Frame)
    ctx.Conn.Write(data)  // 阻塞等待写入完成
    return nil
}

// ✅ 异步写入（非阻塞）
func (h *Handler) Handle(ctx *UserContext) error {
    data := encodeFrame(ctx.Frame)
    ctx.Conn.WriteToOutboundBuffer(data)
    ctx.Conn.WakeWrite()  // 唤醒写事件，异步发送
    return nil
}

优点：
✅ Handler快速返回
✅ 不阻塞事件循环
✅ 吞吐量大幅提升
```

---

### **B. 监控指标**

**1. 队列长度监控**
```go
type Metrics struct {
    // 队列长度
    QueueLength prometheus.Gauge

    // 队列满次数
    QueueFullCount prometheus.Counter

    // 事件处理耗时
    EventDuration prometheus.Histogram
}

func (r *UserReactor) ReportMetrics() {
    // 遍历所有用户队列
    totalLen := 0
    r.userQueues.Range(func(key, value any) bool {
        queue := value.(*UserQueue)
        queueLen := len(queue.queue)
        totalLen += queueLen

        // 告警：队列接近满
        if queueLen > r.queueSize*0.8 {
            log.Warn("queue almost full",
                zap.String("uid", queue.uid),
                zap.Int("len", queueLen))
        }
        return true
    })

    // 上报指标
    metrics.QueueLength.Set(float64(totalLen))
}
```

---

**2. 性能指标**
```
关键指标：

1. 事件处理延迟
   ├─ P50：中位数延迟
   ├─ P99：99分位延迟
   └─ P999：99.9分位延迟

2. 事件处理吞吐
   ├─ QPS：每秒处理事件数
   └─ 峰值QPS

3. 队列健康度
   ├─ 平均队列长度
   ├─ 最大队列长度
   └─ 队列满次数

4. Handler性能
   ├─ 各Handler耗时分布
   ├─ Handler错误率
   └─ Handler超时次数

告警规则：
├─ P99延迟 > 100ms：性能告警
├─ 队列满次数 > 10/min：容量告警
├─ Handler错误率 > 1%：质量告警
└─ QPS下降 > 50%：异常告警
```

---

## 7️⃣ 总结

### **核心要点**

1. **三大事件池职责**
   - UserEventPool：连接事件（高频、轻量）
   - ChannelEventPool：消息事件（中频、中量）
   - PusherEventPool：推送事件（高频、I/O密集）

2. **分片策略**
   - UserEventPool：按UID分片（顺序保证）
   - ChannelEventPool：按ChannelID分片（顺序保证）
   - PusherEventPool：全局队列（无顺序要求）

3. **Worker池模型**
   - 固定数量Worker（8-32个）
   - 任务队列+工作协程
   - 负载均衡（轮询/哈希）

4. **性能优化**
   - 批量处理（减少调度）
   - 对象池复用（减少GC）
   - 异步写入（提高吞吐）

---

### **设计亮点**

| 特性 | 说明 |
|------|------|
| **分层隔离** | 三个池互不影响，故障隔离 |
| **顺序保证** | 同用户/频道事件有序处理 |
| **并行处理** | 不同用户/频道事件并行 |
| **负载均衡** | Worker池均匀分配任务 |
| **可监控** | 丰富的性能指标 |

---

### **下一节预告**

**5.3 事件处理链**
- 事件生成机制
- 事件路由详解
- Handler责任链模式
- 事件转发（跨节点）

---

> **🔗 相关代码**：
> - 事件池接口：`internal/eventbus/eventbus.go`
> - 用户事件池：`internal/user/reactor.go`
> - 频道事件池：`internal/channel/reactor.go`
> - 推送事件池：`internal/pusher/reactor.go`
> - Worker池：`pkg/pool/worker_pool.go`
