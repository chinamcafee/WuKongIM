# 6.3 性能关键路径

> **本节目标**：识别消息发送链路中的热点路径，理解零拷贝、批量写入、异步 ACK 等性能优化策略及其源码实现

---

## 📋 目录
1. [零拷贝与缓冲复用](#零拷贝与缓冲复用)
2. [事件批处理与排队策略](#事件批处理与排队策略)
3. [Raft 批量写入与存储优化](#raft-批量写入与存储优化)
4. [异步 ACK 与重试机制](#异步-ack-与重试机制)
5. [监控指标与调优提示](#监控指标与调优提示)

---

## 1️⃣ 零拷贝与缓冲复用

### **A. 网络层零拷贝路径**
- `internal/server/proto.go:22-165` 使用 `conn.Peek(-1)` 直接获得底层缓冲区切片，解析完成后再统一 `Discard(offset)`，避免额外 `copy`
- `wknet` 的默认连接在读写时都基于 `ringBuffer`：
  - `pkg/wknet/conn.go:205-214` 中 `InboundBuffer` / `OutboundBuffer` 实际类型为 `DefualtBuffer`
  - `pkg/wknet/buffer.go:19-64` 的 `Peek` 提供“查看不取出”的能力，实现真正的零拷贝读取

```go
func (d *DefaultConn) ReadToInboundBuffer() (int, error) {
    readBuffer := d.reactorSub.ReadBuffer
    n, err := d.fd.Read(readBuffer)
    ...
    _, err = d.inboundBuffer.Write(readBuffer[:n]) // ring buffer 内复用
}
```

### **B. 写路径优化**
- `DefaultConn.Write`（`pkg/wknet/conn.go:171-203`）先尝试直接写 socket，若未写完再落入 `outboundBuffer`，减少锁竞争
- `eventbus.User.ConnWrite`（`internal/eventbus/user.go:99-118`）不立即写 socket，而是发布 `EventConnWriteFrame`，由用户事件线程串行写入，有效避免多 goroutine 争抢 fd

> 零拷贝策略贯穿“网络探测 → ring buffer → 串行写 socket”，既减少 CPU 拷贝，又降低锁开销。

---

## 2️⃣ 事件批处理与排队策略

### **A. 批量入队**
- `handleAuthenticatedConn` 将同一批帧合并为 `[]*eventbus.Event` 后一次性投递（`internal/server/proto.go:131-160`）
- `eventbus.User.AddEvents`（`internal/eventbus/user.go:57-65`）逐个写入，再 `Advance` 触发执行，避免频繁唤醒

### **B. EventQueue 限流**
- `EventQueue`（`internal/eventbus/event_queue.go:9-76`）支持 `SliceWithSize(..., maxSize)`，在出队时限制批量总大小，防止单批事件过大拖垮下游
- Channel/Pusher 层均采用相同队列，实现“按 UID / channel 串行”的天然限速

### **C. 多节点转发优化**
- 用户/频道处理器根据 slot leader 判断是否需要跨节点转发（`internal/user/handler/base.go:54-112`、`internal/channel/handler/base.go:35-87`）
- 只在 Leader 节点执行重负载逻辑，其余节点负责简单的事件转发，减小整体冲突面

> 批处理和事件泵机制保证消息处理线程数始终可控，避免 goroutine 爆炸。

---

## 3️⃣ Raft 批量写入与存储优化

### **A. 批量提案**
- 频道层一次 `AppendMessages` 会将多条消息编码为 `ProposeReq`（`pkg/cluster/store/message.go:9-31`）
- 进入 `Raft.ProposeBatchUntilAppliedTimeout`（`pkg/raft/raft/raft_propose.go:53-119`）：
  - Leader 直接将整批日志写入内存队列，再等待 apply
  - Follower 转发给 Leader 并监听 `applyProcess.waitC`
- 批量提案减少了 WAL flush 次数与网络往返，提升吞吐

### **B. Storage Pipeline**
- 同步落盘：WuKongDB 的 `AppendMessages` 使用 Pebble LSM（`pkg/wkdb/message.go:17-94`），并在 Raft 日志应用时落地
- `toPersistMessages` 以结构体赋值方式组装消息（`internal/channel/handler/event_persist.go:154-184`），避免额外编码
- 插件调用放在写入之后（`pluginInvokePersistAfter`），不会阻塞 Raft 关键路径

> 通过“应用前就知道 MessageSeq”的机制，SendAck 可以携带精确序列号，无需额外查询数据库。

---

## 4️⃣ 异步 ACK 与重试机制

### **A. SendAck 分离**
- 频道层在写入完成后才统一发送 ACK（`internal/channel/handler/event_sendack.go:7-33`），用户层不再阻塞等待 Result
- SendAck 与消息发送分离，使得业务逻辑只需等待事件完成即可

### **B. RetryManager 设计**
- `internal/manager/manager_retry.go:16-139`：
  - 使用 `timingwheel` 周期调度，低成本定时
  - 消息按 `messageId` 取模分片，避免全局锁
  - 超过最大重试次数会打印 Trace 并停止，防止压垮系统

### **C. RecvAck 回收**
- 客户端收到消息 → 发送 `Recvack` → 用户层 `recvack`（`internal/user/handler/event_recvack.go:19-71`）
  - 移除重试
  - 可选更新最近会话已读（主设备）
  - 兼容命令消息的非持久化场景

> 这种“先推送→后台重试→收到 ACK 清除”的策略，将 ACK 延迟对主流程的影响降到最低。

---

## 5️⃣ 监控指标与调优提示

### **A. 指标埋点**
- `trace.GlobalTrace.Metrics.App()` 统计 SEND/RECVACK 包数量、字节数（`internal/user/handler/event_onsend.go:130-132`、`internal/user/handler/event_recvack.go:24-32`）
- DB 指标：`trace.GlobalTrace.Metrics.DB().AppendMessagesAdd` 在存储层自增（`pkg/trace/metrics_db.go:963-966`）
- 连接计数、重试队列大小通过 `/varz` 暴露（`internal/api/varz.go:118-126`）

### **B. 调优建议**
1. **RingBuffer 容量**：`options.G.MaxReadBufferSize` 影响最大包体尺寸与内存占用，可根据业务消息大小调整
2. **事件批量大小**：通过配置 `options.G.Event.MaxBatchSize`（若存在）或调整 `EventQueue.limitSize` 相关参数，控制单批事件量
3. **Raft 提案超时**：`options.G.Channel.ProcessTimeout`、`raft.opts.ProposeTimeout` 直接影响写入 RTT，需要与磁盘/网络能力匹配
4. **重试窗口**：`options.G.MessageRetry.WorkerCount`、`MaxCount`、`Interval`（在配置文件中）影响重试吞吐，需与终端 ACK 行为协同
5. **Trace 开关**：`options.G.Logger.TraceOn` 打开后可看到精细日志，但会增加 CPU/IO，建议压测阶段使用

> 性能优化的核心是守住“零拷贝 + 批处理 + 异步确认”三条主线，并通过指标观测及时发现瓶颈。

